<!DOCTYPE html>
<html>

<head>
        <title> Tools for Machine Learning and Sensor Inputs for Gesture Recognition • Nick Arner </title>
        <meta charset="utf-8">
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="apple-touch-icon" sizes="180x180"  href="/favicon.ico"> 
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico"> 
        <link rel="icon" type="image/png" sizes="16x16"  href="/favicon.ico"> 

        
        <meta property="og:url" content="https://nickarner.com" />
        <meta property="og:title" content="Tools for Machine Learning and Sensor Inputs for Gesture Recognition" />
        <meta property="og:description" content="Nick&#39;s Arner&#39;s website." />
        <meta property="og:image" content="https://nickarner.com/card.jpg" />

        
        <meta name="twitter:card" content="summary">
        <meta name="twitter:creator" content="@joodaloop">
        <meta name="twitter:title" content="Tools for Machine Learning and Sensor Inputs for Gesture Recognition">
        <meta name="twitter:description" content="Nick&#39;s Arner&#39;s website." />
        <meta name="twitter:image" content="https://nickarner.com/card.jpg" />

        <meta name="description" content="Nick&#39;s Arner&#39;s website.">
        <link rel="stylesheet" type="text/css" href="/css/styles.css" />
</head>

<body>    
    
<main>
        <div class=left>
                <div class=menu>

	<nav>
		<div class="home">
			<a href="/"> Nick Arner </a>
		</div>
		<a href="/projects_and_work"> Projects </a>
		<a href="/investing"> Investments </a>
		<a href="/notes"> Notes </a>
		<a href="/books"> Books </a>
		<a href="/publications"> Publications </a>
	</nav>
</div>
        </div><div class=content>
                <article>

<h1>Tools for Machine Learning and Sensor Inputs for Gesture Recognition</h1>
      <div class=date>Published on Jun 6, 2017 </div>
      <p>The past several years have seen an explosion in machine learning, including in creative contexts - everything from<a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html" target="_blank" class="external-link" > hallucinating puppyslugs</a>, to<a href="https://magenta.tensorflow.org/welcome-to-magenta" target="_blank" class="external-link" > generating new melodies</a>, machine learning is already beginning to revolutionize the way artists and musicians execute their craft.</p>
<p>My personal interest in the area of machine learning relates to using it to recognize human gestural input via sensors. This interest was sparked from <a href="https://www.nickarner.com/project-soli-alpha-developers-program" target="_blank" class="external-link" >working with Project Soli as a member of the Alpha Developer program</a>.</p>
<p>Sensors offer a bridge between the physical world and the digital. Rich sensor input combined with machine learning allows for new interfaces to be developed that are novel, expressive, and can be configured to a specialized creative task.</p>
<p>In order to make sense of the potential that sensors offer technologists and artists, it&rsquo;s often necessary to utilize machine learning to build a system that can take advantage of a sensor&rsquo;s capabilities. Sensors by themselves aren&rsquo;t always easy to make sense of right away.</p>
<p>With something like an infrared sensor, you can get a<a href="https://github.com/narner/ProximitySensing-With-Chirpino" target="_blank" class="external-link" > simple interactive system</a> working with a sensor and some if-then-else-that type of logic. Something like LIDAR, or a <a href="https://software.intel.com/en-us/realsense/home" target="_blank" class="external-link" >depth-field camera</a>, on the other hand, will have a much larger data footprint - the only way to make sense of the sensor&rsquo;s data is to use machine learning to recognize what patterns are in the real-time data that the sensor is gathering. It’s important to be aware of what type of data a sensor is capable of providing, and whether or not it will be appropriate for the interactive system you are trying to build.</p>
<p>Often, the more complex the data is that the sensor provides, the more interesting things you can do with it.</p>
<p>I wanted to go over some of the open-source tools that are currently available to the creative technology community to leverage the power of sensors for adding gestural interactivity to creative coding projects:</p>
<ul>
<li>Wekinator</li>
<li>Gesture Recognition Toolkit / ofxGrt</li>
<li>ESP</li>
</ul>
<h2 id="wekinator">Wekinator</h2>
<p>The<a href="http://www.wekinator.org/" target="_blank" class="external-link" > Wekinator</a> is a machine-learning middleware program developed by<a href="http://www.doc.gold.ac.uk/~mas01rf/Rebecca_Fiebrink_Goldsmiths/welcome.html" target="_blank" class="external-link" > Dr. Rebecca Fiebrink</a> of Goldsmith&rsquo;s University in London. The basic idea of its use is that it receives data from a sensor via<a href="http://opensoundcontrol.org/" target="_blank" class="external-link" > OSC (open-sound control)</a> from a program that’s acquiring the data from the sensor, such as an Arduino or a Processing sketch. Wekinator is used to train a machine learning system on this incoming data to recognize which gesture has occurred, or to map the start and end times of a gesture to a value that can be used to control a parameter range. These values are sent out from Wekinator via OSC, and can then be received by a program that maps those values to to control audio/visual elements.</p>
<p><a href="http://www.wekinator.org/examples/" target="_blank" class="external-link" >Lots of examples</a> are provided showing how to use a plethora of sensors with the Wekinator, such as a Wii-Mote, Kinect, Leap Motion, etc, and map them to parameters various receiver programs.</p>
<p>What&rsquo;s great about the Wekinator is that you don&rsquo;t have to know much about how machine learning works in order to use it - its strength is in the fact that it&rsquo;s user friendly and easy to experiment with quickly.</p>
<p>If you&rsquo;re interested in exploring how you can use Wekinator to add interactivity to your projects, I highly recommend Dr. Fiebrink&rsquo;s<a href="https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info" target="_blank" class="external-link" > Machine Learning for Artists and Musicians</a> course on Kadenze.</p>
<h2 id="gesture-recognition-toolkit">Gesture Recognition Toolkit</h2>
<p>The<a href="https://github.com/nickgillian/grt" target="_blank" class="external-link" > GRT</a> is a cross-platform toolkit for interfacing with sensors for gesture-recognition systems. It&rsquo;s developed and maintained by<a href="http://nickgillian.com/" target="_blank" class="external-link" > Nick Gillian</a>, who is currently a lead machine learning researcher for Google&rsquo;s<a href="https://atap.google.com/soli/" target="_blank" class="external-link" > Project Soli</a>.</p>
<p>The GRT can be used as a<a href="http://www.nickgillian.com/wiki/pmwiki.php/GRT/GUI" target="_blank" class="external-link" > GUI-based application</a> that acts as middleware between your sensor input via OSC, and a receiver program that reacts to the gesture events detected by your sensor. It’s useage follows the same pattern as the Wekinator.</p>
<p>However, the real benefit of the GRT is how you can write your gesture-recognition pipeline, train your model, and use your code on multiple platforms. This is useful if you want to prototype a gesture-recognition system on your desktop that may need to be deployed on custom, embedded hardware. Additionally, you can write a custom module for the GRT in order to customize your pipeline based on some unique characteristics of the sensor you&rsquo;re using.</p>
<p>Be sure to read Nick’s <a href="http://jmlr.org/papers/volume15/gillian14a/gillian14a.pdf" target="_blank" class="external-link" >paper</a> on the toolkit in the Journal of Machine Learning Research.</p>
<h3 id="ofxgrt">OfxGRT</h3>
<p>The GRT also comes embedded in a wrapper for use in Open Frameworks, a C++ toolkit for creative coding,<a href="https://github.com/nickgillian/ofxGrt" target="_blank" class="external-link" > ofxGrt</a>. This type of wrapper is known as an addon. This makes it extremely easy to integrate into new or existing Open Frameworks projects. Combined with the numerous other Open Frameworks<a href="http://ofxaddons.com/categories" target="_blank" class="external-link" > addons</a>, ofxGrt allows coders to integrate with various types of sensors for adding an interface to physical world with their creative projects.</p>
<h2 id="example-based-sensor-predictions">Example-based Sensor Predictions</h2>
<p>The<a href="https://github.com/damellis/ESP" target="_blank" class="external-link" > Example-based Sensor Predictions</a> project by<a href="http://alumni.media.mit.edu/~mellis/" target="_blank" class="external-link" > David Mellis</a> and<a href="https://www.benzhang.name/" target="_blank" class="external-link" > Ben Zhang</a> was created to make sensor-based machine-learning systems accessible to the Maker and Arduino community. Though the maker community was very familiar with how to work with sensors, fully utilizing their potential for rich interactions really wasn&rsquo;t possible to do in the Arduino ecosystem until this project was developed.</p>
<p>Here’s a short video overview of the project from the creators:</p>
<p> </p>
<p><a href="http://www.youtube.com/watch?v=5nDCG4vkFP0" target="_blank" class="external-link" ><img src="http://img.youtube.com/vi/5nDCG4vkFP0/0.jpg" alt="" title=""  />
</a></p>
<p> </p>
<p>The project is built so that users can interface with sensors via Processing, and the gesture recognition pipeline is built using the GRT. It contains four examples:</p>
<ul>
<li>
<p>Audio Detection</p>
</li>
<li>
<p>Color Detection</p>
</li>
<li>
<p>Gesture Recognition</p>
</li>
<li>
<p>Touché touch detection</p>
<p> </p>
</li>
</ul>
<p>Developers are also given API documentation for how to write their own sensor-based gesture recognition examples as part of the ESP environment.</p>
<p>I&rsquo;ve recently been doing some experiments with the ESP project, and hope to share some of those examples soon.</p>
<p>There’s still a lot of work to be done exploring how new sensing technologies can be leveraged to improve the way that people interact with the devices around them. In order to understand how these technologies may be useful, artists and musicians need to be at the forefront of using them&hellip;pushing sensors and machine learning systems to the limits of how they can be used in expressive contexts. When artists push interfaces to their full potential, everyone benefits as a result - as Bill Buxton said in Artists and the Art of the Luthier, “I also discovered that in the grand scheme of things, there are three levels of design: standard spec., military spec., and artist spec. Most significantly, I learned that the third was the hardest (and most important), but if you could nail it, then everything else was easy.”</p>
<p>The tools described above should encourage artists, musicians, designers, and technologists to explore using sensor-based gesture recognition systems in their creative practice, and imagine ways in which interactivity can be added to new products, pieces, designs, and compositions.</p>

       

                </article>
                <footer> © Copyright Nicholas Arner, 2023  </footer>
        </div>
        
</main>

        
</body>
</html>